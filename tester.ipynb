{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\condaana\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\condaana\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\condaana\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in d:\\condaana\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in d:\\condaana\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\condaana\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\condaana\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\condaana\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\condaana\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\condaana\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\condaana\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\condaana\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\condaana\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\condaana\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\condaana\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\condaana\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\anudeep bura\\appdata\\roaming\\python\\python311\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\condaana\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\condaana\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\condaana\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in d:\\condaana\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\condaana\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in d:\\condaana\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\condaana\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in d:\\condaana\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in d:\\condaana\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install nltk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show', 'me', 'all', 'records', 'from', 'the', 'users', 'table', 'where', 'age', 'is', 'greater', 'than', '25']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"Show me all records from the users table where age is greater than 25\"\n",
    "tokens = preprocess_text(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Show me all records from the users table where age is greater than 25\"\n",
    "\n",
    "def recognize_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "entities = recognize_entities(text)\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show: VERB, ROOT\n",
      "me: PRON, dative\n",
      "all: DET, det\n",
      "records: NOUN, dobj\n",
      "from: ADP, prep\n",
      "the: DET, det\n",
      "users: NOUN, compound\n",
      "table: NOUN, pobj\n",
      "where: SCONJ, advmod\n",
      "age: NOUN, nsubj\n",
      "is: AUX, relcl\n",
      "greater: ADJ, amod\n",
      "than: ADP, quantmod\n",
      "25: NUM, npadvmod\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process a sample text\n",
    "text = \"Show me all records from the users table where age is greater than 25\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print named entities, part-of-speech tags, and dependencies\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_}, {token.dep_}\")\n",
    "\n",
    "# Print recognized entities\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text}: {entity.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy_lookups_data' has no attribute 'get_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize lookups\u001b[39;00m\n\u001b[0;32m     13\u001b[0m lookups \u001b[38;5;241m=\u001b[39m Lookups()\n\u001b[1;32m---> 14\u001b[0m lookups\u001b[38;5;241m.\u001b[39madd_table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlexeme_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mspacy_lookups_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_table\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlexeme_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     15\u001b[0m lookups\u001b[38;5;241m.\u001b[39madd_table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemma_lookup\u001b[39m\u001b[38;5;124m\"\u001b[39m, spacy_lookups_data\u001b[38;5;241m.\u001b[39mget_table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemma_lookup\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Add the lookups data to the vocabulary of the model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'spacy_lookups_data' has no attribute 'get_table'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.pipeline.textcat import Config\n",
    "from spacy.lookups import Lookups\n",
    "from spacy.util import load_config\n",
    "from spacy.lookups import load_lookups\n",
    "import spacy_lookups_data\n",
    "\n",
    "# Load your spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize lookups\n",
    "lookups = Lookups()\n",
    "lookups.add_table(\"lexeme_norm\", spacy_lookups_data.get_table(\"lexeme_norm\"))\n",
    "lookups.add_table(\"lemma_lookup\", spacy_lookups_data.get_table(\"lemma_lookup\"))\n",
    "\n",
    "# Add the lookups data to the vocabulary of the model\n",
    "nlp.vocab.lookups.add_lookups(lookups)\n",
    "\n",
    "# Add the lookups data to the vocabulary of the model\n",
    "# Add text classifier to the pipeline\n",
    "config = {\"threshold\": 0.5}\n",
    "textcat = nlp.add_pipe(\"textcat\", config=config)\n",
    "\n",
    "# Add labels to the text classifier\n",
    "textcat.add_label(\"SELECT\")\n",
    "textcat.add_label(\"INSERT\")\n",
    "textcat.add_label(\"UPDATE\")\n",
    "textcat.add_label(\"DELETE\")\n",
    "\n",
    "# Prepare training data\n",
    "train_data = [\n",
    "    (\"Show me all records from the users table where age is greater than 25\", {\"cats\": {\"SELECT\": 1, \"INSERT\": 0, \"UPDATE\": 0, \"DELETE\": 0}}),\n",
    "    (\"Add a new user with name John and age 30\", {\"cats\": {\"SELECT\": 0, \"INSERT\": 1, \"UPDATE\": 0, \"DELETE\": 0}}),\n",
    "    (\"Update the age of user with id 1 to 31\", {\"cats\": {\"SELECT\": 0, \"INSERT\": 0, \"UPDATE\": 1, \"DELETE\": 0}}),\n",
    "    (\"Delete the user with id 1\", {\"cats\": {\"SELECT\": 0, \"INSERT\": 0, \"UPDATE\": 0, \"DELETE\": 1}}),\n",
    "]\n",
    "\n",
    "# Training loop\n",
    "optimizer = nlp.initialize()\n",
    "for epoch in range(10):\n",
    "    losses = {}\n",
    "    for text, annotations in train_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], losses=losses, sgd=optimizer)\n",
    "    print(f\"Epoch {epoch} - Loss: {losses}\")\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"intent_classifier\")\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"intent_classifier\")\n",
    "\n",
    "# Predict the intent of a new query\n",
    "def predict_intent(text):\n",
    "    doc = nlp(text)\n",
    "    intent = max(doc.cats, key=doc.cats.get)\n",
    "    confidence = doc.cats[intent]\n",
    "    return intent, confidence\n",
    "\n",
    "# Example usage\n",
    "text = \"Show me all records from the users table where age is greater than 25\"\n",
    "intent, confidence = predict_intent(text)\n",
    "print(f\"Intent: {intent}, Confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'columns_match' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 58\u001b[0m\n\u001b[0;32m     50\u001b[0m queries \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow me all records from the users table\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdate the age of user with id 5 to 35\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd a new user with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and age 28\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelete the user with id 3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m ]\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m---> 58\u001b[0m     components \u001b[38;5;241m=\u001b[39m \u001b[43mextract_query_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomponents[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 38\u001b[0m, in \u001b[0;36mextract_query_components\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     35\u001b[0m     conditions_match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(delete_conditions_pattern, query, re\u001b[38;5;241m.\u001b[39mIGNORECASE)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Assign matched groups to columns and conditions\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcolumns_match\u001b[49m:\n\u001b[0;32m     39\u001b[0m     columns \u001b[38;5;241m=\u001b[39m columns_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conditions_match:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'columns_match' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_query_components(query):\n",
    "    # Define regex patterns for different query components\n",
    "    intent_pattern = r'^\\s*(SELECT|UPDATE|INSERT INTO|DELETE FROM)\\b'\n",
    "    select_columns_pattern = r'SELECT\\s+(.+?)\\s+FROM'\n",
    "    update_columns_pattern = r'UPDATE\\s+\\w+\\s+SET\\s+(.+?)\\s+(WHERE|$)'\n",
    "    insert_columns_pattern = r'INSERT INTO\\s+\\w+\\s*\\((.+?)\\)\\s*VALUES'\n",
    "    delete_conditions_pattern = r'DELETE FROM\\s+\\w+\\s+WHERE\\s+(.+?)\\s*;?$'\n",
    "    select_conditions_pattern = r'WHERE\\s+(.+?)\\s*;?$'\n",
    "    update_conditions_pattern = r'WHERE\\s+(.+?)\\s*;?$'\n",
    "\n",
    "    # Initialize extracted components\n",
    "    intent = None\n",
    "    columns = None\n",
    "    conditions = None\n",
    "\n",
    "    # Extract intent\n",
    "    intent_match = re.search(intent_pattern, query, re.IGNORECASE)\n",
    "    if intent_match:\n",
    "        intent = intent_match.group(1).upper()\n",
    "    \n",
    "    # Extract columns and conditions based on the intent\n",
    "    if intent == 'SELECT':\n",
    "        columns_match = re.search(select_columns_pattern, query, re.IGNORECASE)\n",
    "        conditions_match = re.search(select_conditions_pattern, query, re.IGNORECASE)\n",
    "    elif intent == 'UPDATE':\n",
    "        columns_match = re.search(update_columns_pattern, query, re.IGNORECASE)\n",
    "        conditions_match = re.search(update_conditions_pattern, query, re.IGNORECASE)\n",
    "    elif intent == 'INSERT INTO':\n",
    "        columns_match = re.search(insert_columns_pattern, query, re.IGNORECASE)\n",
    "        conditions_match = None  # INSERT usually doesn't have conditions\n",
    "    elif intent == 'DELETE FROM':\n",
    "        columns_match = None  # DELETE usually doesn't specify columns\n",
    "        conditions_match = re.search(delete_conditions_pattern, query, re.IGNORECASE)\n",
    "\n",
    "    # Assign matched groups to columns and conditions\n",
    "    if columns_match:\n",
    "        columns = columns_match.group(1).split(', ')\n",
    "    if conditions_match:\n",
    "        conditions = conditions_match.group(1)\n",
    "\n",
    "    return {\n",
    "        'intent': intent,\n",
    "        'columns': columns,\n",
    "        'conditions': conditions\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "queries = [\n",
    "    \"Show me all records from the users table\",\n",
    "    \"Update the age of user with id 5 to 35\",\n",
    "    \"Add a new user with name 'Alice' and age 28\",\n",
    "    \"Delete the user with id 3\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    components = extract_query_components(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Intent: {components['intent']}\")\n",
    "    print(f\"Columns: {components['columns']}\")\n",
    "    print(f\"Conditions: {components['conditions']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities found in the text:\n",
      "Text: Alice, Label: PERSON\n",
      "Text: age 14, Label: DATE\n",
      "Text: 9, Label: CARDINAL\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_and_print_entities(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    # Print entities\n",
    "    if entities:\n",
    "        print(\"Entities found in the text:\")\n",
    "        for entity in entities:\n",
    "            print(f\"Text: {entity[0]}, Label: {entity[1]}\")\n",
    "    else:\n",
    "        print(\"No entities found in the text.\")\n",
    "\n",
    "# Example text\n",
    "text = \"Add a new student with name 'Alice', age 14, class 9, section A.\"\n",
    "\n",
    "# Extract and print entities\n",
    "extract_and_print_entities(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add new student name Alice  age 14 grade 9 section \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Sample text\n",
    "text = \"Add a new student with name 'Alice', age 14, grade 9, section 'A'\"\n",
    "def remove_stop_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    gg=' '.join(filtered_words)\n",
    "    gg=gg.replace('id','ID')\n",
    "    g=gg.replace(\"'\",\"\").replace(\",\",\"\").replace(\"  \",\" \")\n",
    "    return g\n",
    "print(remove_stop_words(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update name Zane student ID 29\n",
      "Entities found in the text:\n",
      "Text: Update, Label: ORG\n",
      "Text: Zane, Label: PERSON\n",
      "Text: 29, Label: CARDINAL\n",
      "Text: name Zane, Label: NAME\n",
      "Text: ID 29, Label: ID\n"
     ]
    }
   ],
   "source": [
    "##using this for themain thinggg!!!\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the patterns for matching\n",
    "patterns = [\n",
    "    {\"label\": \"CLASS\", \"pattern\": [{\"LOWER\": \"class\"}, {\"IS_DIGIT\": True}]},\n",
    "    {\"label\": \"SECTION\", \"pattern\": [{\"LOWER\": \"section\"}, {\"IS_ALPHA\": True}]},\n",
    "    {\"label\": \"NAME\", \"pattern\": [{\"LOWER\": \"name\"}, {\"IS_ALPHA\": True}]},\n",
    "        {\"label\": \"NAMES\", \"pattern\": [{\"LOWER\": \"names\"}, {\"IS_ALPHA\": True}]},\n",
    "        {\"label\": \"ID\", \"pattern\": [{\"LOWER\": \"id\"}, {\"IS_DIGIT\": True}]},\n",
    "\n",
    "    {\"label\": \"AGE\", \"pattern\": [{\"LOWER\": \"age\"}, {\"IS_DIGIT\": True}]},\n",
    "    {\"label\": \"ID\", \"pattern\": [{\"LOWER\": \"id\"}, {\"IS_DIGIT\": True}]},\n",
    "    {\"label\": \"ENGLISH\", \"pattern\": [{\"LOWER\": \"english\"}, {\"IS_DIGIT\": True},{\"IS_ALPHA\": True}]},\n",
    "    {\"label\": \"HINDI\", \"pattern\": [{\"LOWER\": \"hindi\"}, {\"IS_DIGIT\": True}]},\n",
    "    {\"label\": \"SCIENCE\", \"pattern\": [{\"LOWER\": \"science\"}, {\"IS_DIGIT\": True}]},\n",
    "    {\"label\": \"GRADE\", \"pattern\": [{\"LOWER\": \"grade\"}, {\"IS_DIGIT\": True}]},\n",
    "\n",
    "\n",
    "    \n",
    "]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for pattern in patterns:\n",
    "    matcher.add(pattern[\"label\"], [pattern[\"pattern\"]])\n",
    "\n",
    "# Initialize the phrase matcher for known entities\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# List of known entities (e.g., sections, subjects, common names)\n",
    "known_entities = [ \"english\", \"hindi\", \"science\"]\n",
    "patterns = [nlp(text) for text in known_entities]\n",
    "phrase_matcher.add(\"KNOWN_ENTITIES\", None, *patterns)\n",
    "\n",
    "def extract_and_print_entities(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entities using NER\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    # Extract entities using rule-based matching\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        entities.append((span.text, nlp.vocab.strings[match_id]))\n",
    "    \n",
    "    # Extract entities using phrase matching\n",
    "    phrase_matches = phrase_matcher(doc)\n",
    "    for match_id, start, end in phrase_matches:\n",
    "        span = doc[start:end]\n",
    "        entities.append((span.text, \"KNOWN_ENTITY\"))\n",
    "    \n",
    "    # Print entities\n",
    "    if entities:\n",
    "        print(\"Entities found in the text:\")\n",
    "        for entity in entities:\n",
    "            print(f\"Text: {entity[0]}, Label: {entity[1]}\")\n",
    "    else:\n",
    "        print(\"No entities found in the text.\")\n",
    "\n",
    "# Example text\n",
    "text = \"Update name to 'Zane' for student with id 29\"\n",
    "gg=remove_stop_words(text)\n",
    "# Extract and print entities\n",
    "print(gg)\n",
    "extract_and_print_entities(gg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load a pre-trained NER model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m ner_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbmdz/bert-large-cased-finetuned-conll03-english\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Example text\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd a new student with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, age 14, class 9, section A.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Anudeep Bura\\.conda\\envs\\languageprocessing\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\Anudeep Bura\\.conda\\envs\\languageprocessing\\Lib\\site-packages\\transformers\\pipelines\\base.py:234\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    240\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained NER model\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Example text\n",
    "text = \"Add a new student with name 'Alice', age 14, class 9, section A.\"\n",
    "\n",
    "# Extract entities using the pre-trained model\n",
    "entities = ner_pipeline(text)\n",
    "for entity in entities:\n",
    "    print(entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty lines have been removed from dataset.txt\n"
     ]
    }
   ],
   "source": [
    "# Function to remove empty lines from a text file\n",
    "def remove_empty_lines(file_path):\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Filter out empty lines\n",
    "    cleaned_lines = [line for line in lines if line.strip()]\n",
    "    \n",
    "    # Write the cleaned lines back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(cleaned_lines)\n",
    "\n",
    "# Specify the input file\n",
    "input_file = 'dataset.txt'\n",
    "\n",
    "# Remove empty lines from the specified file\n",
    "remove_empty_lines(input_file)\n",
    "\n",
    "print(f\"Empty lines have been removed from {input_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Function to read and parse the text file\n",
    "def parse_text_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        current_entry = {}\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Query:\"):\n",
    "                if current_entry:\n",
    "                    data.append(current_entry)\n",
    "                current_entry = {\"Query\": line.strip().split(\"Query:\")[1].strip().strip('\"')}\n",
    "            elif line.startswith(\"Intent:\"):\n",
    "                current_entry[\"Intent\"] = line.strip().split(\"Intent:\")[1].strip()\n",
    "            elif line.startswith(\"Columns:\"):\n",
    "                current_entry[\"Columns\"] = line.strip().split(\"Columns:\")[1].strip()\n",
    "            elif line.startswith(\"Conditions:\"):\n",
    "                current_entry[\"Conditions\"] = line.strip().split(\"Conditions:\")[1].strip()\n",
    "        if current_entry:\n",
    "            data.append(current_entry)\n",
    "    return data\n",
    "\n",
    "# Function to write data to a CSV file\n",
    "def write_to_csv(data, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = [\"Query\", \"Intent\", \"Columns\", \"Conditions\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Specify the input and output files\n",
    "input_file = 'dataset.txt'\n",
    "output_file = 'output.csv'\n",
    "\n",
    "# Parse the text file and write to CSV\n",
    "data = parse_text_file(input_file)\n",
    "write_to_csv(data, output_file)\n",
    "\n",
    "print(f\"Data has been written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add a new student with name 'alice ' , age 14 , grade 9 , section ' a '\n",
      "insert into students ( name , age , grade , section ) values ( 'alice ' , 14 , 9 , ' a ' ) ;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example\n",
    "nl_query = \"Add a new student with name 'Alice', age 14, grade 9, section 'A'\"\n",
    "sql_query = \"INSERT INTO students (name, age, grade, section) VALUES ('Alice', 14, 9, 'A');\"\n",
    "print(preprocess_text(nl_query))\n",
    "print(preprocess_text(sql_query))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneration\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(train_data):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Tokenize the input and output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anudeep Bura\\.conda\\envs\\languageprocessing\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1507\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1507\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anudeep Bura\\.conda\\envs\\languageprocessing\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1495\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1493\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "def train_model(train_data):\n",
    "    # Tokenize the input and output\n",
    "    input_texts = [example['input'] for example in train_data]\n",
    "    target_texts = [example['output'] for example in train_data]\n",
    "\n",
    "    input_encodings = tokenizer(input_texts, truncation=True, padding=True, max_length=512)\n",
    "    target_encodings = tokenizer(target_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Convert to tensors and create dataset\n",
    "    input_ids = input_encodings['input_ids']\n",
    "    attention_mask = input_encodings['attention_mask']\n",
    "    labels = target_encodings['input_ids']\n",
    "\n",
    "    dataset = CustomDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "    # Train the model\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "# Example training data\n",
    "train_data = [\n",
    "    {'input': \"Add a new student with name 'Alice', age 14, grade 9, section 'A'\", 'output': \"INSERT INTO students (name, age, grade, section) VALUES ('Alice', 14, 9, 'A');\"},\n",
    "    # Add more training examples\n",
    "]\n",
    "\n",
    "train_model(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(natural_language_query):\n",
    "    inputs = tokenizer.encode(\"translate English to SQL: \" + natural_language_query, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs)\n",
    "    sql_query = tokenizer.decode(outputs[0])\n",
    "    return sql_query\n",
    "\n",
    "# Test the model\n",
    "test_query = \"Add a new student with name 'Alice', age 14, grade 9, section 'A'\"\n",
    "print(generate_sql(test_query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The animals are running in the fields. Students are getting good grades in their classes. We have multiple names in the list.\n",
      "Lemmatized text: The animal are running in the field . Students are getting good grade in their class . We have multiple name in the list .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def rems(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text\n",
    "print(\"Original text:\", text)\n",
    "print(\"Lemmatized text:\", lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['age', 'section']\n",
      "Conditions: is greater than is\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample text\n",
    "text = \"Select the names and ages of students where age is greater than 14 and section is A\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define potential columns and condition keywords\n",
    "column_keywords = {\"name\", \"age\", \"class\", \"section\", \"english\", \"hindi\", \"science\"}\n",
    "condition_keywords = {\"older\", \"younger\", \"greater\", \"less\", \"than\", \"above\", \"below\", \"equals\", \"equal\", \"is\", \"in\", \"section\"}\n",
    "\n",
    "# Extract potential columns and conditions\n",
    "columns = []\n",
    "conditions = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.text.lower() in column_keywords:\n",
    "        columns.append(token.text)\n",
    "    elif token.text.lower() in condition_keywords or token.ent_type_ in {\"CARDINAL\", \"NUMBER\"}:\n",
    "        conditions.append(token.text)\n",
    "    elif token.ent_type_ in {\"ORG\"} and token.head.text.lower() in condition_keywords:\n",
    "        conditions.append(token.text)\n",
    "\n",
    "# Join the conditions into a single string\n",
    "conditions_str = ' '.join(conditions)\n",
    "\n",
    "# Print the extracted columns and conditions\n",
    "print(\"Columns:\", columns)\n",
    "print(\"Conditions:\", conditions_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['section']\n",
      "Conditions: older than 14 in\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Show the names and ages of students who are older than 14 and in section A\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Define potential columns and condition keywords\n",
    "column_keywords = {\"name\", \"age\", \"class\", \"section\", \"english\", \"hindi\", \"science\"}\n",
    "condition_keywords = {\"older\", \"younger\", \"greater\", \"less\", \"than\", \"above\", \"below\", \"equals\", \"equal\", \"in\", \"section\"}\n",
    "\n",
    "# Extract potential columns and conditions\n",
    "columns = []\n",
    "conditions = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token.lower() in column_keywords:\n",
    "        columns.append(token)\n",
    "    elif token.lower() in condition_keywords or token.isdigit():\n",
    "        conditions.append(token)\n",
    "\n",
    "# Join the conditions into a single string\n",
    "conditions_str = ' '.join(conditions)\n",
    "\n",
    "# Print the extracted columns and conditions\n",
    "print(\"Columns:\", columns)\n",
    "print(\"Conditions:\", conditions_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def extract_and_convert_conditions(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Define condition keywords and their SQL equivalents\n",
    "    condition_mapping = {\n",
    "        \"greater\": \">\",\n",
    "        \"less\": \"<\",\n",
    "        \"older\": \">\",\n",
    "        \"younger\": \"<\",\n",
    "        \"than\": \"\",\n",
    "        \"is\": \"=\",\n",
    "    }\n",
    "    \n",
    "    # Extract conditions\n",
    "    conditions = []\n",
    "    column = None\n",
    "    comparator = None\n",
    "    value = None\n",
    "    \n",
    "    for token in doc:\n",
    "        # Check if the token is part of the condition keywords or is a number\n",
    "        if token.text.lower() in condition_mapping:\n",
    "            if token.text.lower() in [\"greater\", \"less\", \"older\", \"younger\"]:\n",
    "                comparator = condition_mapping[token.text.lower()]\n",
    "            elif token.text.lower() == \"than\":\n",
    "                # Skip the 'than' keyword\n",
    "                continue\n",
    "            elif token.text.lower() == \"is\":\n",
    "                comparator = \"=\"\n",
    "        elif token.ent_type_ == \"CARDINAL\" or token.ent_type_ == \"NUMBER\":\n",
    "            value = token.text\n",
    "        elif token.text.lower() == \"age\":\n",
    "            column = \"age\"\n",
    "        elif token.text.lower() == \"section\":\n",
    "            column = \"section\"\n",
    "        # Add more column mappings here if needed\n",
    "    \n",
    "    # Construct SQL condition\n",
    "    if column and comparator and value:\n",
    "        sql_condition = f\"{column} {comparator} {value}\"\n",
    "        conditions.append(sql_condition)\n",
    "    \n",
    "    return conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Show the names and ages of students who are older than 14 and in section A\n",
      "SQL Conditions: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text=\"Show the names and ages of students who are older than 14 and in section A\"\n",
    "conditions = extract_and_convert_conditions(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"SQL Conditions: {conditions}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Show the names and ages of students who are older than 14 and in section A\n",
      "SQL Conditions: []\n",
      "\n",
      "Text: Select students where age is greater than 18 and less than 21\n",
      "SQL Conditions: []\n",
      "\n",
      "Text: Find all students younger than 15\n",
      "SQL Conditions: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to extract and convert conditions to SQL format\n",
    "def extract_and_convert_conditions(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Define condition keywords and their SQL equivalents\n",
    "    condition_mapping = {\n",
    "        \"greater\": \">\",\n",
    "        \"less\": \"<\",\n",
    "        \"older\": \">\",\n",
    "        \"younger\": \"<\",\n",
    "        \"than\": \"\",\n",
    "        \"is\": \"=\",\n",
    "        \"in\": \"in\",\n",
    "    }\n",
    "    \n",
    "    # Initialize variables\n",
    "    conditions = []\n",
    "    column = None\n",
    "    comparator = None\n",
    "    value = None\n",
    "    found_column = False\n",
    "    \n",
    "    for token in doc:\n",
    "        token_text = token.text.lower()\n",
    "        \n",
    "        if token_text in condition_mapping:\n",
    "            if token_text in [\"greater\", \"less\", \"older\", \"younger\"]:\n",
    "                comparator = condition_mapping[token_text]\n",
    "            elif token_text in [\"than\", \"is\"]:\n",
    "                comparator = condition_mapping[token_text]\n",
    "        elif token.ent_type_ in [\"CARDINAL\", \"NUMBER\"]:\n",
    "            value = token.text\n",
    "        elif token_text in [\"age\", \"section\", \"class\", \"name\"]:\n",
    "            column = token_text\n",
    "            found_column = True\n",
    "        elif found_column and comparator and value:\n",
    "            # Create the SQL condition\n",
    "            sql_condition = f\"{column} {comparator} {value}\"\n",
    "            conditions.append(sql_condition)\n",
    "            # Reset variables after processing\n",
    "            column = None\n",
    "            comparator = None\n",
    "            value = None\n",
    "            found_column = False\n",
    "    \n",
    "    return conditions\n",
    "\n",
    "# Sample texts\n",
    "texts = [\n",
    "    \"Show the names and ages of students who are older than 14 and in section A\",\n",
    "    \"Select students where age is greater than 18 and less than 21\",\n",
    "    \"Find all students younger than 15\",\n",
    "]\n",
    "\n",
    "# Extract and convert conditions from sample texts\n",
    "for text in texts:\n",
    "    conditions = extract_and_convert_conditions(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"SQL Conditions: {conditions}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Sentence preprocessing (replace with your actual NLP pipeline)\n",
    "sentence = \"Show the names and ages of students who are older than 14 and in section A\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Simplified entity extraction (adapt based on your NLP pipeline's output)\n",
    "entities = []\n",
    "for i, (token, pos) in enumerate(pos_tags):\n",
    "    if pos in [\"ADJ\", \"NOUN\"] and (i + 1 < len(pos_tags) and pos_tags[i + 1][0] == \"PREP\"):\n",
    "        entities.append(\" \".join([token, pos_tags[i + 1][0], pos_tags[i + 2][0]]))\n",
    "\n",
    "print(entities)  # Output: [\"age > 14\", \"section = A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Show the names and ages of students who are older than 14 and in section A\n",
      "Comparisons: []\n",
      "\n",
      "Text: Select students where age is greater than 18 and less than 21\n",
      "Comparisons: []\n",
      "\n",
      "Text: Find all students younger than 15\n",
      "Comparisons: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to extract comparison entities\n",
    "def extract_comparisons(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Define comparison keywords\n",
    "    comparison_keywords = {\"greater than\": \">\", \"less than\": \"<\", \"older than\": \">\", \"younger than\": \"<\"}\n",
    "    \n",
    "    # Initialize list to store comparisons\n",
    "    comparisons = []\n",
    "    \n",
    "    # Iterate through the tokens and identify comparison keywords\n",
    "    for token in doc:\n",
    "        # Check if the token and its following token match any comparison keyword\n",
    "        if token.text.lower() in comparison_keywords:\n",
    "            # Check the next token to form the full comparison phrase\n",
    "            next_token = doc[token.i + 1] if token.i + 1 < len(doc) else None\n",
    "            comparison_phrase = f\"{token.text.lower()} {next_token.text.lower()}\" if next_token else token.text.lower()\n",
    "            \n",
    "            if comparison_phrase in comparison_keywords:\n",
    "                comparisons.append(comparison_phrase)\n",
    "    \n",
    "    return comparisons\n",
    "\n",
    "# Sample texts\n",
    "texts = [\n",
    "    \"Show the names and ages of students who are older than 14 and in section A\",\n",
    "    \"Select students where age is greater than 18 and less than 21\",\n",
    "    \"Find all students younger than 15\",\n",
    "]\n",
    "\n",
    "# Extract comparisons from sample texts\n",
    "for text in texts:\n",
    "    comparisons = extract_comparisons(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Comparisons: {comparisons}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Show the names and ages of students who are older than 14 and in section A\"\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text+\"|\"+ent.label_+\"|\"+spacy.explain(ent.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load pre-trained SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define patterns for comparison and keywords\n",
    "comparison_patterns = {\n",
    "    'greater_than': r'(greater|older|more)\\s*than\\s*(\\d+)',\n",
    "    'less_than': r'(less|fewer|lower)\\s*than\\s*(\\d+)',\n",
    "    'equals': r'(is|are|equals|same)\\s*as\\s*([A-Za-z0-9]+)',\n",
    "    'in_section': r'in\\s*section\\s*([A-Za-z])',\n",
    "}\n",
    "\n",
    "def extract_conditions(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    conditions = []\n",
    "\n",
    "    # Extract key phrases and numeric values\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['NOUN', 'ADJ'] and token.dep_ in ['attr', 'nsubj']:\n",
    "            entity = token.text.lower()\n",
    "            if 'age' in entity or 'marks' in entity:\n",
    "                for pattern_name, pattern in comparison_patterns.items():\n",
    "                    match = re.search(pattern, sentence)\n",
    "                    if match:\n",
    "                        if pattern_name == 'greater_than':\n",
    "                            conditions.append(f\"{entity} > {match.group(2)}\")\n",
    "                        elif pattern_name == 'less_than':\n",
    "                            conditions.append(f\"{entity} < {match.group(2)}\")\n",
    "                        elif pattern_name == 'equals':\n",
    "                            conditions.append(f\"{entity} = {match.group(2)}\")\n",
    "                        elif pattern_name == 'in_section':\n",
    "                            conditions.append(f\"{entity} = {match.group(1)}\")\n",
    "    \n",
    "    return ' AND '.join(conditions)\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"Show the names and ages of students who are older than 14 and in section A\"\n",
    "sentence2 = \"Find students with english marks greater than 45 and age should be less than 20\"\n",
    "\n",
    "print(extract_conditions(sentence1))  # Output: \"age > 14 AND section = A\"\n",
    "print(extract_conditions(sentence2))  # Output: \"english marks > 45 AND age < 20\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section = A\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define patterns for extracting conditions\n",
    "patterns = {\n",
    "    'greater_than': r'(\\d+)\\s*(greater|older|more)\\s*than\\s*(\\d+)',\n",
    "    'less_than': r'(\\d+)\\s*(less|fewer|lower)\\s*than\\s*(\\d+)',\n",
    "    'equals': r'is\\s*(equal|same)\\s*as\\s*([\\w\\d]+)',\n",
    "    'in_section': r'in\\s*section\\s*([A-Za-z])'\n",
    "}\n",
    "\n",
    "def extract_conditions(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    conditions = []\n",
    "    \n",
    "    # Extract conditions using regex patterns\n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        matches = re.findall(pattern, sentence)\n",
    "        for match in matches:\n",
    "            if pattern_name == 'greater_than':\n",
    "                conditions.append(f\"{match[0]} > {match[2]}\")\n",
    "            elif pattern_name == 'less_than':\n",
    "                conditions.append(f\"{match[0]} < {match[2]}\")\n",
    "            elif pattern_name == 'equals':\n",
    "                conditions.append(f\"{match[1]} = {match[2]}\")\n",
    "            elif pattern_name == 'in_section':\n",
    "                conditions.append(f\"section = {match[0]}\")\n",
    "    \n",
    "    return ' AND '.join(conditions)\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Show the names and ages of students who are older than 14 and in section A\"\n",
    "print(extract_conditions(sentence))  # Output: \"age > 14 AND section = A\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Get', 'VB'), ('the', 'DT'), ('names', 'NNS'), ('of', 'IN'), ('students', 'NNS'), ('who', 'WP'), ('scored', 'VBD'), ('more', 'JJR'), ('than', 'IN'), ('80', 'CD'), ('in', 'IN'), ('Math', 'NN')]\n",
      "SELECT name FROM students WHERE math_score > 80\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Get the names of students who scored more than 80 in Math\"\n",
    "\n",
    "# Tokenize and POS tagging\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "print(tagged)\n",
    "\n",
    "# Define rules to map POS tags to SQL components (simplified example)\n",
    "def map_to_sql(tagged_tokens):\n",
    "    if \"scored\" in tokens and \"more\" in tokens:\n",
    "        return \"SELECT name FROM students WHERE math_score > 80\"\n",
    "    # Add more rules based on your dataset and needs\n",
    "    return \"\"\n",
    "\n",
    "sql_query = map_to_sql(tagged)\n",
    "print(sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(naturallang):\n",
    "    pattern = r'(\\d+)(st|nd|rd|th)'\n",
    "    naturallang=re.sub(pattern, r'\\1', naturallang)\n",
    "    naturallang=naturallang.replace(\"grade\",\"class\")\n",
    "    words = nltk.word_tokenize(naturallang)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    words = word_tokenize(lemmatized_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.remove(\"a\")\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    gg=' '.join(filtered_words)\n",
    "    gg=gg.replace('id','ID')\n",
    "    g=gg.replace(\"'\",\"\").replace(\",\",\"\").replace(\"  \",\" \")\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Find', 'NN'), ('record', 'NN'), ('where', 'WRB'), ('the', 'DT'), ('score', 'NN'), ('is', 'VBZ'), ('greater', 'JJR'), ('than', 'IN'), ('80', 'CD'), ('and', 'CC'), ('the', 'DT'), ('price', 'NN'), ('is', 'VBZ'), ('less', 'JJR'), ('than', 'IN'), ('100', 'CD')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anudeep Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# Ensure that you have the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Find record where the score is greater than 80 and the price is less than 100\"\n",
    "# Tokenize and POS tagging\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(tagged_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comparison phrases and their variants\n",
    "comparison_phrases = [\n",
    "    \"greater than\", \"more than\", \"less than\", \"fewer than\",\n",
    "    \"equal to\", \"not equal to\", \"between\", \"in\"\n",
    "]\n",
    "\n",
    "# Define keywords related to comparisons\n",
    "keywords = [\"score\", \"value\", \"amount\", \"price\", \"quantity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Entity: ind record where the **score** is **greater than** 80 and the price is less than\n",
      "Comparison Entity: ater than 80 and the **price** is **less than** 100\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_comparison_entities(sentence):\n",
    "    comparison_entities = []\n",
    "\n",
    "    # Check for comparison phrases in the sentence\n",
    "    for phrase in comparison_phrases:\n",
    "        pattern = r'\\b' + re.escape(phrase) + r'\\b'\n",
    "        matches = re.finditer(pattern, sentence, flags=re.IGNORECASE)\n",
    "        \n",
    "        for match in matches:\n",
    "            start = match.start()\n",
    "            end = match.end()\n",
    "\n",
    "            # Extract context around the comparison phrase\n",
    "            context = sentence[max(0, start - 30):min(len(sentence), end + 30)]\n",
    "            \n",
    "            # Check for keywords in the context\n",
    "            for keyword in keywords:\n",
    "                keyword_pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
    "                if re.search(keyword_pattern, context, flags=re.IGNORECASE):\n",
    "                    # Highlight the comparison phrase and keyword\n",
    "                    highlighted_context = re.sub(keyword_pattern, f'**{keyword}**', context)\n",
    "                    highlighted_context = re.sub(pattern, f'**{phrase}**', highlighted_context)\n",
    "                    comparison_entities.append(highlighted_context)\n",
    "                    break\n",
    "\n",
    "    return comparison_entities\n",
    "\n",
    "# Extract comparison entities\n",
    "comparison_entities = extract_comparison_entities(sentence)\n",
    "\n",
    "# Print results\n",
    "for entity in comparison_entities:\n",
    "    print(\"Comparison Entity:\", entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: ['Find']\n"
     ]
    }
   ],
   "source": [
    "def chunk_entities(tagged_tokens):\n",
    "    chunked = ne_chunk(tagged_tokens)\n",
    "    entities = []\n",
    "    \n",
    "    for subtree in chunked:\n",
    "        if isinstance(subtree, nltk.Tree):\n",
    "            entity = \" \".join(word for word, tag in subtree.leaves())\n",
    "            entities.append(entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Chunk named entities\n",
    "chunked_entities = chunk_entities(tagged_tokens)\n",
    "print(\"Named Entities:\", chunked_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and POS Tags: [('Find', 'NN'), ('records', 'NNS'), ('where', 'WRB'), ('the', 'DT'), ('score', 'NN'), ('is', 'VBZ'), ('greater', 'JJR'), ('than', 'IN'), ('80', 'CD'), ('and', 'CC'), ('the', 'DT'), ('price', 'NN'), ('is', 'VBZ'), ('less', 'JJR'), ('than', 'IN'), ('100', 'CD'), ('.', '.'), ('John', 'NNP'), ('Doe', 'NNP'), ('bought', 'VBD'), ('300', 'CD'), ('shares', 'NNS'), ('of', 'IN'), ('Acme', 'NNP'), ('Corp', 'NNP'), ('in', 'IN'), ('2021', 'CD'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anudeep Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Ensure that you have the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"Find records where the score is greater than 80 and the price is less than 100. John Doe bought 300 shares of Acme Corp in 2021.\"\n",
    "\n",
    "# Tokenize and POS tagging\n",
    "tokens = word_tokenize(text)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(\"Tokens and POS Tags:\", tagged_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: less than 100, Label: CARDINAL\n",
      "Entity: John Doe, Label: PERSON\n",
      "Entity: 300, Label: CARDINAL\n",
      "Entity: Acme Corp, Label: ORG\n",
      "Entity: 2021, Label: DATE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f96e8b0d28384ea7b0b320d60ab03c1e-0\" class=\"displacy\" width=\"4600\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Find</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">records</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">where</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">score</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">greater</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">than</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">80</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">price</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">less</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">than</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">100.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">John</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">Doe</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">bought</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">300</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">shares</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">Acme</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">Corp</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">2021.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M205.0,441.5 L213.0,429.5 197.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-1\" stroke-width=\"2px\" d=\"M420,439.5 C420,264.5 910.0,264.5 910.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-2\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-3\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-4\" stroke-width=\"2px\" d=\"M245,439.5 C245,177.0 915.0,177.0 915.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,441.5 L923.0,429.5 907.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-5\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,264.5 1435.0,264.5 1435.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-6\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,352.0 1430.0,352.0 1430.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,441.5 L1287,429.5 1303,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-7\" stroke-width=\"2px\" d=\"M945,439.5 C945,177.0 1440.0,177.0 1440.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,441.5 L1448.0,429.5 1432.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-8\" stroke-width=\"2px\" d=\"M70,439.5 C70,89.5 1620.0,89.5 1620.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,441.5 L1628.0,429.5 1612.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-9\" stroke-width=\"2px\" d=\"M1820,439.5 C1820,352.0 1955.0,352.0 1955.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,441.5 L1812,429.5 1828,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-10\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,352.0 2130.0,352.0 2130.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,441.5 L1987,429.5 2003,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-11\" stroke-width=\"2px\" d=\"M70,439.5 C70,2.0 2150.0,2.0 2150.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2150.0,441.5 L2158.0,429.5 2142.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-12\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,264.5 2660.0,264.5 2660.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,441.5 L2337,429.5 2353,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-13\" stroke-width=\"2px\" d=\"M2520,439.5 C2520,352.0 2655.0,352.0 2655.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,441.5 L2512,429.5 2528,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-14\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,177.0 2665.0,177.0 2665.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2665.0,441.5 L2673.0,429.5 2657.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-15\" stroke-width=\"2px\" d=\"M2870,439.5 C2870,352.0 3005.0,352.0 3005.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,441.5 L2862,429.5 2878,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-16\" stroke-width=\"2px\" d=\"M3045,439.5 C3045,352.0 3180.0,352.0 3180.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,441.5 L3037,429.5 3053,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-17\" stroke-width=\"2px\" d=\"M3395,439.5 C3395,352.0 3530.0,352.0 3530.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3395,441.5 L3387,429.5 3403,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-18\" stroke-width=\"2px\" d=\"M3220,439.5 C3220,264.5 3535.0,264.5 3535.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3535.0,441.5 L3543.0,429.5 3527.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-19\" stroke-width=\"2px\" d=\"M3570,439.5 C3570,352.0 3705.0,352.0 3705.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3705.0,441.5 L3713.0,429.5 3697.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-20\" stroke-width=\"2px\" d=\"M3920,439.5 C3920,352.0 4055.0,352.0 4055.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3920,441.5 L3912,429.5 3928,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-21\" stroke-width=\"2px\" d=\"M3745,439.5 C3745,264.5 4060.0,264.5 4060.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4060.0,441.5 L4068.0,429.5 4052.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-22\" stroke-width=\"2px\" d=\"M3220,439.5 C3220,177.0 4240.0,177.0 4240.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4240.0,441.5 L4248.0,429.5 4232.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-23\" stroke-width=\"2px\" d=\"M4270,439.5 C4270,352.0 4405.0,352.0 4405.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f96e8b0d28384ea7b0b320d60ab03c1e-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4405.0,441.5 L4413.0,429.5 4397.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Entities: ['nd records where the score is greater than 80 and the price is less than', 'ater than 80 and the price is less than 100. John Doe bought 300 shar', 'ought 300 shares of Acme Corp in 2021.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load spaCy's pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
    "\n",
    "# Visualize dependency parsing (requires Jupyter environment)\n",
    "from spacy import displacy\n",
    "\n",
    "# Visualize the dependency parsing\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "\n",
    "# Define comparison phrases\n",
    "comparison_phrases = [\n",
    "    \"greater than\", \"more than\", \"less than\", \"fewer than\",\n",
    "    \"equal to\", \"not equal to\", \"between\", \"in\"\n",
    "]\n",
    "\n",
    "# Extract comparison entities\n",
    "def extract_comparison_entities(text):\n",
    "    comparison_entities = []\n",
    "    \n",
    "    for phrase in comparison_phrases:\n",
    "        pattern = r'\\b' + re.escape(phrase) + r'\\b'\n",
    "        matches = re.finditer(pattern, text, flags=re.IGNORECASE)\n",
    "        \n",
    "        for match in matches:\n",
    "            start = match.start()\n",
    "            end = match.end()\n",
    "            \n",
    "            # Extract context around the comparison phrase\n",
    "            context = text[max(0, start - 30):min(len(text), end + 30)]\n",
    "            comparison_entities.append(context.strip())\n",
    "    \n",
    "    return comparison_entities\n",
    "\n",
    "# Extract comparison entities\n",
    "comparison_entities = extract_comparison_entities(text)\n",
    "print(\"Comparison Entities:\", comparison_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column found: age\n",
      "Column found: grade\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Load a spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the PhraseMatcher with the shared vocabulary\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# List of column names\n",
    "columns = [\"name\", \"id\", \"age\", \"grade\", \"section\", \"english\", \"hindi\", \"science\"]\n",
    "patterns = [nlp(text) for text in columns]\n",
    "\n",
    "# Add the patterns to the PhraseMatcher\n",
    "phrase_matcher.add(\"COLUMN\", None, *patterns)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Find all students with age greater than 18 and grade A\")\n",
    "\n",
    "# Apply the PhraseMatcher to the doc\n",
    "column_matches = phrase_matcher(doc)\n",
    "\n",
    "# Print the matches\n",
    "for match_id, start, end in column_matches:\n",
    "    span = doc[start:end]\n",
    "    print(f\"Column found: {span.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition found: age greater than 18\n",
      "Condition found: grade equal to A\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define patterns for conditions\n",
    "patterns = [\n",
    "    [{\"LOWER\":{\"IN\":columns}},{\"LOWER\": {\"IN\": [\"greater\", \"less\", \"equal\"]}}, {\"LOWER\": \"than\"}, {\"IS_DIGIT\": True}],  # e.g., \"greater than 18\"\n",
    "    [{\"LOWER\":{\"IN\":columns}},{\"LOWER\": \"equal\"}, {\"LOWER\": \"to\"}, {\"IS_DIGIT\": True}],\n",
    "      [{\"LOWER\":{\"IN\":columns}},{\"LOWER\": \"equal\"}, {\"LOWER\": \"to\"}, {\"IS_ALPHA\": True}]  # e.g., \"equal to 10\"\n",
    "]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher.add(\"CONDITION\", patterns)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Find all students with age greater than 18 and grade equal to A\")\n",
    "\n",
    "# Apply the Matcher to the doc\n",
    "condition_matches = matcher(doc)\n",
    "\n",
    "# Print the matches\n",
    "for match_id, start, end in condition_matches:\n",
    "    span = doc[start:end]\n",
    "    print(f\"Condition found: {span.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Match found: name\n",
      "Column Match found: age\n",
      "Column Match found: id\n",
      "Column Match found: age\n",
      "Condition Match found: d = 123\n",
      "Condition Match found: age > 20\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "\n",
    "# Load a spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the PhraseMatcher and Matcher\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# List of column names\n",
    "columns = [\"name\", \"age\", \"id\", \"grade\", \"section\", \"english\", \"hindi\", \"science\"]\n",
    "column_patterns = [nlp(text) for text in columns]\n",
    "phrase_matcher.add(\"COLUMN\", None, *column_patterns)\n",
    "\n",
    "# Define patterns for conditions\n",
    "condition_patterns = [\n",
    "    [{\"IS_ALPHA\": True}, {\"IS_ASCII\": True, \"OP\": \"?\"}, {\"IS_DIGIT\": True}],  # e.g., \"id = 123\"\n",
    "    [{\"IS_ALPHA\": True}, {\"IS_ASCII\": True, \"OP\": \"?\"}, {\"IS_DIGIT\": True, \"OP\": \"+\"}],  # e.g., \"age > 20\"\n",
    "    [{\"IS_ALPHA\": True}, {\"IS_ASCII\": True, \"OP\": \"?\"}, {\"LIKE_NUM\": True}]  # e.g., \"age = 20.5\"\n",
    "]\n",
    "matcher.add(\"CONDITION\", condition_patterns)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"SELECT name, age FROM students WHERE id = 123 AND age > 20\")\n",
    "\n",
    "# Apply both matchers to the doc\n",
    "column_matches = phrase_matcher(doc)\n",
    "condition_matches = matcher(doc)\n",
    "\n",
    "# Print column matches\n",
    "for match_id, start, end in column_matches:\n",
    "    span = doc[start:end]\n",
    "    print(f\"Column Match found: {span.text}\")\n",
    "\n",
    "# Print condition matches\n",
    "for match_id, start, end in condition_matches:\n",
    "    span = doc[start:end]\n",
    "    print(f\"Condition Match found: {span.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher, Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need this\n",
    "def preprocess(naturallang):\n",
    "    naturallang=naturallang.replace(\"marks\",\"\")\n",
    "    naturallang=naturallang.replace(\"are\",\"\")\n",
    "    naturallang=naturallang.replace(\"to\",\"\")\n",
    "    pattern = r'(\\d+)(st|nd|rd|th)'\n",
    "    naturallang=re.sub(pattern, r'\\1', naturallang)\n",
    "    naturallang=naturallang.replace(\"grade\",\"class\")\n",
    "    words = nltk.word_tokenize(naturallang)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    # words = word_tokenize(lemmatized_text)\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # stop_words.remove(\"a\")\n",
    "    # filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # gg=' '.join(filtered_words)\n",
    "    gg=lemmatized_text\n",
    "    gg=gg.replace('id','ID')\n",
    "    # g=gg.replace(\"'\",\"\").replace(\",\",\"\").replace(\"  \",\" \")\n",
    "    return gg\n",
    "#need this!!\n",
    "english_words = set(words.words())\n",
    "english_words.remove(\"a\")\n",
    "def is_english_word(word):\n",
    "    return word.lower() in english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"studentdatset.csv\")\n",
    "queries=df[\"Query\"]\n",
    "intents=df[\"Intent\"]\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(intents)\n",
    "X_train, X_test, y_train, y_test = train_test_split(queries, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train model\n",
    "model = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict intent\n",
    "def predict_intent(query):\n",
    "    intent_idx = model.predict([query])[0]\n",
    "    return label_encoder.inverse_transform([intent_idx])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getentities(text):\n",
    "# Load a spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Initialize the PhraseMatcher for columns\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "    columns = [\"name\", \"id\", \"age\", \"grade\", \"section\", \"english\", \"hindi\", \"science\"]\n",
    "    patterns = [nlp(text) for text in columns]\n",
    "    phrase_matcher.add(\"COLUMN\", None, *patterns)\n",
    "\n",
    "    # Initialize the Matcher for conditions\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    condition_patterns = [\n",
    "        [{\"LOWER\":{\"IN\":columns}},{\"LOWER\": {\"IN\": [\"greater\", \"less\", \"equal\"]}}, {\"LOWER\": \"than\"}, {\"IS_DIGIT\": True}],  # e.g., \"greater than 18\"\n",
    "        [{\"LOWER\":{\"IN\":columns}},{\"LOWER\": \"equal\"}, {\"LOWER\": \"to\"}, {\"IS_DIGIT\": True}],\n",
    "        [{\"LOWER\":{\"IN\":columns}},{\"LOWER\": \"equal\"}, {\"LOWER\": \"to\"}, {\"IS_ALPHA\": True}],\n",
    "        [{\"LOWER\": {\"IN\": [\"older\", \"younger\", \"equal\"]}}, {\"LOWER\": \"than\"}, {\"IS_DIGIT\": True}],\n",
    "        [{\"LOWER\":'id'},{\"IS_DIGIT\": True}]\n",
    "    ]\n",
    "    dbcolumns=[\"age\",\"name\",\"section\",\"class\",\"id\",\"english\",\"hindi\",\"science\"]\n",
    "    patterns=[]\n",
    "    for colm in dbcolumns:\n",
    "        patterns.append([{\"LOWER\": colm}, {\"IS_ALPHA\":True}])\n",
    "        patterns.append([{\"LOWER\": colm}, {\"IS_DIGIT\":True}])\n",
    "    condition_patterns+=patterns\n",
    "\n",
    "\n",
    "    matcher.add(\"CONDITION\", condition_patterns)\n",
    "\n",
    "    # Process a text\n",
    "    # text=\"Get names of students whose english marks are greater than 40\"\n",
    "\n",
    "    pretext=preprocess(text)\n",
    "\n",
    "    doc = nlp(pretext)\n",
    "    # Apply the PhraseMatcher and Matcher to the doc\n",
    "    column_matches = phrase_matcher(doc)\n",
    "    condition_matches = matcher(doc)\n",
    "    finalcolumns=[]\n",
    "    # Print the column matches\n",
    "    for match_id, start, end in column_matches:\n",
    "        span = doc[start:end]\n",
    "        finalcolumns.append(span.text)\n",
    "\n",
    "    finalconditions=[]\n",
    "    # Print the condition matches\n",
    "    for match_id, start, end in condition_matches:\n",
    "        span = doc[start:end]\n",
    "        word=span.text.split(\" \")\n",
    "        if len(word)==2: \n",
    "            if is_english_word(word[1]):\n",
    "                pass\n",
    "            else:\n",
    "                # print(word[1])\n",
    "                if not word[1].isdigit:\n",
    "                    word[1]=\"\\'\"+word[1]+\"\\'\"\n",
    "                finalconditions.append(\"=\".join(word))\n",
    "        else:\n",
    "            finalconditions.append(span.text)\n",
    "    for i in finalcolumns:\n",
    "        for j in finalconditions:\n",
    "            if i in j:\n",
    "                finalcolumns.remove(i)\n",
    "    relations={\n",
    "        \"greater than\":\">\",\n",
    "        \"less than\":\"<\",\n",
    "        \"equal to\":\"=\",\n",
    "        \"is equal to\":\"=\",\n",
    "        \"older than\":\"age>\",\n",
    "        \"younger than\":\"age<\"\n",
    "    }\n",
    "    for i in range(0,len(finalconditions)):\n",
    "        for key, value in relations.items():\n",
    "            finalconditions[i]=finalconditions[i].replace(key,value)\n",
    "    return finalcolumns, finalconditions\n",
    "# print(finalcolumns)\n",
    "# print(finalconditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update class 9 for student with ID 25\n",
      "UPDATE\n",
      "[]\n",
      "['class=9', 'ID=25']\n",
      "UPDATE students SET class=9 WHERE id=25;\n"
     ]
    }
   ],
   "source": [
    "text=\"Update grade to 9 for student with id 25\"\n",
    "print(preprocess(text))\n",
    "intent=predict_intent(text)\n",
    "print(intent)\n",
    "finalcolumns,finalconditions=getentities(text)\n",
    "print(finalcolumns)\n",
    "print(finalconditions)\n",
    "print(genquery(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genquery(text):\n",
    "    query=\"\"\n",
    "    if intent==\"SELECT\":\n",
    "        cols=\"\"\n",
    "        condi=\"\"\n",
    "        for i in range(0,len(finalcolumns)-1):\n",
    "            cols+=i+\",\"\n",
    "        cols+=finalcolumns[len(finalcolumns)-1]\n",
    "        for i in range(0,len(finalconditions)-1):\n",
    "            condi+=i+\" and\"\n",
    "        condi+=finalconditions[len(finalconditions)-1]\n",
    "\n",
    "        \n",
    "        query=\"SELECT \"+cols+\" FROM students\"\n",
    "        if condi!=\"\":\n",
    "            query+=\" WHERE \"+condi+\";\"\n",
    "    elif intent==\"UPDATE\":\n",
    "        colnames=\"\"\n",
    "        val=\"\"\n",
    "        for i in range(len(finalconditions)-1):\n",
    "            val+=finalconditions[i]\n",
    "        colnames+=finalconditions[len(finalconditions)-1]\n",
    "        query=\"UPDATE students SET \"+ val+ \" WHERE \"+ colnames+\";\"\n",
    "    elif intent==\"DELETE\":\n",
    "        gg=\"\"\n",
    "        for i in finalconditions:\n",
    "            i=i.split(\"=\")\n",
    "            if i[1].isdigit():\n",
    "                gg+=i[0]+\"=\"+i[1]\n",
    "            else:\n",
    "                gg+=i[0]+\"=\"+\"\\'\"+i[1]+\"\\'\"\n",
    "        query=\"DELETE FROM students WHERE \"+gg+\";\"\n",
    "\n",
    "    elif intent==\"INSERT\":\n",
    "        colnames=\"\"\n",
    "        val=\"\"\n",
    "        for u in range(0,len(finalconditions)):\n",
    "            if u==len(finalconditions)-1:\n",
    "                g=finalconditions[u].split(\"=\")\n",
    "                colnames+=g[0]\n",
    "                if g[1].isdigit():\n",
    "                    val+=g[1]+\",\"\n",
    "                else:\n",
    "                    val+=\"\\'\"+g[1]+\"\\'\"+\",\"\n",
    "            else:\n",
    "                g=finalconditions[u].split(\"=\")\n",
    "                colnames+=g[0]+\",\"\n",
    "                if g[1].isdigit():\n",
    "                    val+=g[1]+\",\"\n",
    "                else:\n",
    "                    val+=\"\\'\"+g[1]+\"\\'\"+\",\"\n",
    "        query=\"INSERT INTO students (\"+colnames+\") VALUES (\"+val+\");\"\n",
    "    query=query.replace('ID','id')  \n",
    "    return query\n",
    "# print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no ggs\n"
     ]
    }
   ],
   "source": [
    "if not'9'.isdigit():\n",
    "    print(\"ggs\")\n",
    "else:\n",
    "    print(\"no ggs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
