{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_query = '''\n",
    "INSERT INTO students (name, age, class, section, english, hindi, science)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "'''\n",
    "\n",
    "# Step 4: Iterate over the rows of the dataframe and insert data\n",
    "for index, row in data.iterrows():\n",
    "    cursor.execute(insert_query, (\n",
    "        row['name'],\n",
    "        row['age'],\n",
    "        row['class'],\n",
    "        row['section'],\n",
    "        row['english'],\n",
    "        row['hindi'],\n",
    "        row['science']\n",
    "    ))\n",
    "\n",
    "# Step 5: Commit the transaction\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'databasetable.csv'\n",
    "data = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Anudeep\n",
      "[nltk_data]     Bura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"studentdatset.csv\")\n",
    "# Prepare data\n",
    "\n",
    "queries=df[\"Query\"]\n",
    "intents=df[\"Intent\"]\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(intents)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(queries, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train model\n",
    "model = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict intent\n",
    "def predict_intent(query):\n",
    "    intent_idx = model.predict([query])[0]\n",
    "    return label_encoder.inverse_transform([intent_idx])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rule-based matching of text patterns\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "dbcolumns=[\"age\",\"name\",\"section\",\"class\",\"id\",\"english\",\"hindi\",\"science\"]\n",
    "patterns=[]\n",
    "for colm in dbcolumns:\n",
    "    patterns.append({\"label\": colm.upper(), \"pattern\": [{\"LOWER\": colm}, {\"IS_ALPHA\":True}]})\n",
    "    patterns.append({\"label\": colm.upper(), \"pattern\": [{\"LOWER\": colm}, {\"IS_DIGIT\":True}]})\n",
    "\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for pattern in patterns:\n",
    "    matcher.add(pattern[\"label\"], [pattern[\"pattern\"]])\n",
    "\n",
    "# Initialize the phrase matcher for known entities\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "#Aho-Corasick algorithm\n",
    "known_entities = [ \"english\", \"hindi\", \"science\"]\n",
    "patterns = [nlp(text) for text in known_entities]\n",
    "phrase_matcher.add(\"KNOWN_ENTITIES\", None, *patterns)\n",
    "\n",
    "def extract_and_print_entities(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract entities using NER\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    # Extract entities using rule-based matching\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        entities.append((span.text, nlp.vocab.strings[match_id]))\n",
    "    \n",
    "    # Extract entities using phrase matching\n",
    "    phrase_matches = phrase_matcher(doc)\n",
    "    for match_id, start, end in phrase_matches:\n",
    "        span = doc[start:end]\n",
    "        entities.append((span.text, \"KNOWN_ENTITY\"))\n",
    "    \n",
    "    # Print entities\n",
    "    if entities:\n",
    "        print(\"Entities found in the text:\")\n",
    "        for entity in entities:\n",
    "            print(f\"Text: {entity[0]}, Label: {entity[1]}\")\n",
    "        return entities\n",
    "    else:\n",
    "        print(\"No entities found in the text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(naturallang):\n",
    "    pattern = r'(\\d+)(st|nd|rd|th)'\n",
    "    naturallang=re.sub(pattern, r'\\1', naturallang)\n",
    "    naturallang=naturallang.replace(\"grade\",\"class\")\n",
    "    words = nltk.word_tokenize(naturallang)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    words = word_tokenize(lemmatized_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.remove(\"a\")\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    gg=' '.join(filtered_words)\n",
    "    gg=gg.replace('id','ID')\n",
    "    g=gg.replace(\"'\",\"\").replace(\",\",\"\").replace(\"  \",\" \")\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add a new student name Vera  age 15 class 10 section B \n",
      "Entities found in the text:\n",
      "Text: Vera, Label: ORG\n",
      "Text: age 15, Label: DATE\n",
      "Text: name Vera, Label: NAME\n",
      "Text: age 15, Label: AGE\n",
      "Text: class 10, Label: CLASS\n",
      "Text: section B, Label: SECTION\n"
     ]
    }
   ],
   "source": [
    "naturallang=\"Add a new student with name 'Vera', age 15, grade 10, section 'B'\"\n",
    "naturallang=preprocess(naturallang)\n",
    "print(naturallang)\n",
    "intent=predict_intent(naturallang)\n",
    "entities=extract_and_print_entities(naturallang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add a new student name Vera  age 15 class 10 section B \n",
      "[('name Vera', 'NAME'), ('age 15', 'AGE'), ('class 10', 'CLASS'), ('section B', 'SECTION')]\n"
     ]
    }
   ],
   "source": [
    "print(naturallang)\n",
    "columns=[\"CLASS\",\"SECTION\",\"NAME\",\"NAMES\",\"ID\",\"AGE\",\"ENGLISH\",\"HINDI\",\"SCIENCE\",\"GRADE\"]\n",
    "req=[]\n",
    "for entitie in entities:\n",
    "    if(entitie[1] in columns):\n",
    "        req.append(entitie)\n",
    "print(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name Vera'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name=Vera', 'age=15', 'class=10', 'section=B']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "finalcolm=[]\n",
    "finalcondi=[]\n",
    "for i in req:\n",
    "    if \" \" in i[0]:\n",
    "        finalcondi.append(i[0].replace(\" \",\"=\"))\n",
    "    else:\n",
    "        finalcolm.append(i[0])\n",
    "print(finalcondi)\n",
    "print(finalcolm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO students (name,age,class,section) VALUES ('Vera',15,10,'B',);\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query=\"\"\n",
    "if intent==\"SELECT\":\n",
    "    cols=\"\"\n",
    "    condi=\"\"\n",
    "    for i in finalcolm:\n",
    "        cols+=i\n",
    "\n",
    "        if i!=len(finalcolm-1):\n",
    "            cols+=\",\"\n",
    "    for j in range(len(finalcondi)-1):\n",
    "        print(j)\n",
    "        k=finalcondi[j].split(\"=\")\n",
    "        cols+=k[0]\n",
    "        \n",
    "        if j!=len(finalcondi)-2:\n",
    "            cols+=\",\"\n",
    "    con=finalcondi[len(finalcondi)-1].split(\"=\")\n",
    "    condi+=con[0]\n",
    "    if con[1].isdigit():\n",
    "        print(con[1].isdigit)\n",
    "        \n",
    "        condi+=\"=\"+con[1]\n",
    "    else:\n",
    "        condi+=\"=\\'\"+con[1]+\"\\'\"\n",
    "\n",
    "    query=\"SELECT \"+cols+\" FROM students\"\n",
    "    if condi!=\"\":\n",
    "        query+=\" WHERE \"+condi+\";\"\n",
    "elif intent==\"UPDATE\":\n",
    "    colnames=\"\"\n",
    "    val=\"\"\n",
    "    for i in range(0,len(finalcondi)):\n",
    "        if i==len(finalcondi)-1:\n",
    "            i=finalcondi[i].split(\"=\")\n",
    "            if i[1].isdigit:\n",
    "                colnames+=i[0]+\"=\"+i[1]\n",
    "            else:\n",
    "                colnames+=i[0]+\"=\"+\"\\'\"+i[1]+\"\\'\"\n",
    "        elif i==len(finalcondi)-2:\n",
    "            i=finalcondi[i].split(\"=\")\n",
    "            if i[1].isdigit:\n",
    "                val+=i[0]+\"=\"+i[1]\n",
    "            else:\n",
    "                val+=i[0]+\"=\"+\"\\'\"+i[1]+\"\\'\"\n",
    "        else:\n",
    "            i=finalcondi[i].split(\"=\")\n",
    "            if i[1].isdigit():\n",
    "                val+=i[0]+\"=\"+i[1]+\",\"\n",
    "            else:\n",
    "                val+=i[0]+\"=\"+\"\\'\"+i[1]+\"\\'\"+\",\"\n",
    "    query=\"UPDATE students SET \"+ val+ \" WHERE \"+ colnames+\";\"\n",
    "elif intent==\"DELETE\":\n",
    "    gg=\"\"\n",
    "    for i in finalcondi:\n",
    "        i=i.split(\"=\")\n",
    "        if i[1].isdigit():\n",
    "            gg+=i[0]+\"=\"+i[1]\n",
    "        else:\n",
    "            gg+=i[0]+\"=\"+\"\\'\"+i[1]+\"\\'\"\n",
    "    query=\"DELETE FROM students WHERE \"+gg+\";\"\n",
    "\n",
    "elif intent==\"INSERT\":\n",
    "    colnames=\"\"\n",
    "    val=\"\"\n",
    "    for u in range(0,len(finalcondi)):\n",
    "        if u==len(finalcondi)-1:\n",
    "            g=finalcondi[u].split(\"=\")\n",
    "            colnames+=g[0]\n",
    "            if g[1].isdigit():\n",
    "                val+=g[1]+\",\"\n",
    "            else:\n",
    "                val+=\"\\'\"+g[1]+\"\\'\"+\",\"\n",
    "        else:\n",
    "            g=finalcondi[u].split(\"=\")\n",
    "            colnames+=g[0]+\",\"\n",
    "            if g[1].isdigit():\n",
    "                val+=g[1]+\",\"\n",
    "            else:\n",
    "                val+=\"\\'\"+g[1]+\"\\'\"+\",\"\n",
    "    query=\"INSERT INTO students (\"+colnames+\") VALUES (\"+val+\");\"\n",
    "print(query)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A',)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace with your actual database credentials\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',        # e.g., 'localhost' or '127.0.0.1'\n",
    "    user='root',    # e.g., 'root'\n",
    "    password='Marsmover$123',# e.g., 'your_password'\n",
    "    database='school'        # Your database name\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT section FROM students WHERE name='John Doe'\")\n",
    "cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "languageprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
